---
title: "Airline Tweets NLP Analysis"
author: 
date: 
output: word_document
---

This document shows the results of basic natural language processing (NLP) analysis on Twitter tweets about major US airlines scraped from the site during part of February 2015. Specifically, I create a word cloud and conduct sentiment analysis.

Contributors to the data set were asked to classify positive, negative, and neutral tweets.
Thus, for each tweet, I have the 'correct' answer for sentiment analysis purposes.

The data can be found at the URL below. To find the dataset, search for 'Airline' on the page.  
I specifically use the 16,000 row dataset uploaded on February 12, 2015 by CrowdFlower.  
I assume the upload date is incorrect as the data includes tweets from after 2/12/2015...

https://www.crowdflower.com/data-for-everyone/

Note that the actual dataset only contains 14,640 rows. I'm not sure where the discrepancy comes from, but it doesn't affect the analysis.

```{r import_and_prep, include = F, echo = F}
  
  # Load packages.
  library(data.table)
  library(tm)
  library(SnowballC)
  library(wordcloud)
  library(tm.plugin.sentiment)
  library(e1071)
  library(FSelector)

  # Import data.
  tweet_data = data.table(read.csv('Airline-Sentiment-2-w-AA.csv', 
                                  header = T, 
                                  stringsAsFactors = F))
  
  # Change tweet text to all lowercase.
  tweet_data$text = tolower(tweet_data$text)
  
  # Only include needed columns.
  tweet_data = tweet_data[, .(airline_sentiment, text)]
  
```

### Word Cloud:

Below is a word cloud with the 50 most frequently used words (technically stems) in the tweet data. 
The larger and darker a word, the more frequently it was used.

```{r word_cloud, include = T, echo = F}
  
  # Create corpus of tweet data ('tweet_data$text').
  tweet_corpus = Corpus(VectorSource(tweet_data$text))
  
  # Remove punctuation from 'tweet_corpus'.
  wordcloud_tweet_corpus = tm_map(tweet_corpus, 
                                  removePunctuation)
  
  # Remove extra whitespace from 'tweet_corpus'.
  wordcloud_tweet_corpus = tm_map(wordcloud_tweet_corpus, 
                                  stripWhitespace)
  
  # Remove numbers from 'tweet_corpus'.
  wordcloud_tweet_corpus = tm_map(wordcloud_tweet_corpus, 
                                  removeNumbers)
  
  # Remove (English) stopwords and airline names from 'tweet_corpus'.
  # I want to exclude airline names in the word cloud and at least one airline name is likely to be in every tweet.
  wordcloud_tweet_corpus = tm_map(wordcloud_tweet_corpus, 
                                  removeWords, 
                                  c('virginamerica', 'united', 'southwestair', 'jetblue', 'usairways', 'americanair', 
                                    stopwords('english')))
  
  # Perform stemming on 'tweet_corpus'.
  wordcloud_tweet_corpus = tm_map(wordcloud_tweet_corpus, 
                                  stemDocument)
  
  # Plot wordcloud.
  wordcloud(wordcloud_tweet_corpus,
            max.words = 50,
            random.order = F, 
            colors = c('lightskyblue', 'cornflowerblue', 'blue', 'blue4'))
  
```

### Sentiment Analysis:

I conduct sentiment analysis in 2 ways:  
* Lexicon based (with pre-provided lists of positive and negative terms)   
* Naive Bayes Classification Model

**Lexicon Based**:

```{r sentiment_analysis_lexicon_based, include = T, echo = F}
  
  # Conduct data cleaning and compute sentiment analysis scores for 'tweet_data'.
  # Specifically, the text cleaning that occurs is making the text lowercase, removing punctuation, 
  # removing numbers, removing (English) stopwords, removing whitespace, stemming words, and removing words with less than 3 characters.
  # I do this process on the unmodified 'tweet_corpus' instead of 'wordcloud_tweet_corpus' which has already been preprocessed to some extent.
  sa_tweet_data = score(tweet_corpus)
  
  # Get sentiment analysis scores for each tweet.
  # I list the score metric defintions below (found this info on slide 36: http://statmath.wu.ac.at/courses/SNLP/Presentations/DA-Sentiment.pdf)
  # In these definitions, N is the total number of words in a document and p[n] is the number of positive[negative] words in a document.
  # Positive and negative words are included in a pre-defined list called 'dic_gi'.
  # There are 5 score metrics computed:
  # polarity = (p - n) / (p + n)
  # subjectivity = (n + p) / N
  # pos_refs_per_ref = p / N
  # neg_refs_per_ref = n / N
  # senti_diffs_per_ref = (p - n) / N
  sa_tweet_data = meta(sa_tweet_data)
  sa_tweet_data = data.table(sa_tweet_data)
  
  # Add 'sa_tweet_data$polarity' to 'tweet_data'.
  tweet_data$polarity = sa_tweet_data$polarity
  
  # Translate polarity into a 'predicted_sentiment_lexicon_based'.
  # If 'polarity' is less[greater] than 0, the tweet will be predicted to have negative[positive] sentiment.
  # If 'polarity' is 0, the tweet will be predicted to have neutral sentiment.
  tweet_data$predicted_sentiment_lexicon_based = ifelse(tweet_data$polarity < 0, 'negative', 
                                          ifelse(tweet_data$polarity == 0, 'neutral', 'positive'))
  
```

Before modeling, I make the text lowercase, remove punctuation, remove numbers, remove (English) stopwords, remove whitespace, stem words, and remove words with less than 3 characters.   
The scoring metric I use is polarity and is computed as: (p - n) / (p + n)

p[n] is the number of positive[negative] words in a tweet. These positive and negative words come in a pre-defined list.

For each tweet, if polarity is less[greater] than 0, the tweet will be classified as negative[positive].   
Tweets with a polarity of 0 are classified as neutral.

Using these definitions, `r nrow(tweet_data[airline_sentiment == predicted_sentiment_lexicon_based])` out of `r nrow(tweet_data)` (`r round(100 * nrow(tweet_data[airline_sentiment == predicted_sentiment_lexicon_based]) / nrow(tweet_data), 2)`)% of tweets are classified correctly.

Results by tweet classification category:   
`r nrow(tweet_data[airline_sentiment == predicted_sentiment_lexicon_based & airline_sentiment == 'positive'])` out of `r nrow(tweet_data[airline_sentiment == 'positive'])` (`r round(100 * nrow(tweet_data[airline_sentiment == predicted_sentiment_lexicon_based & airline_sentiment == 'positive']) / nrow(tweet_data[airline_sentiment == 'positive']), 2)`)% of positive tweets are classified correctly.

`r nrow(tweet_data[airline_sentiment == predicted_sentiment_lexicon_based & airline_sentiment == 'neutral'])` out of `r nrow(tweet_data[airline_sentiment == 'neutral'])` (`r round(100 * nrow(tweet_data[airline_sentiment == predicted_sentiment_lexicon_based & airline_sentiment == 'neutral']) / nrow(tweet_data[airline_sentiment == 'neutral']), 2)`)% of neutral tweets are classified correctly.

`r nrow(tweet_data[airline_sentiment == predicted_sentiment_lexicon_based & airline_sentiment == 'negative'])` out of `r nrow(tweet_data[airline_sentiment == 'negative'])` (`r round(100 * nrow(tweet_data[airline_sentiment == predicted_sentiment_lexicon_based & airline_sentiment == 'negative']) / nrow(tweet_data[airline_sentiment == 'negative']), 2)`)% of negative tweets are classified correctly.

Further Exploration:  

These results are not great. Results might be improved by using a different (possibly custom) list of positively and negatively associated words. Since these tweets are directed at airlines, the lists should probably include air travel specific terms. Results might also improve by using different polarity cutoffs. Perhaps tweets with scores that are slightly above/below zero should be classified as neutral. There are many variations to explore here. Finally, a different scoring metric might yield better results.

**Naive Bayes Classification Model**:

```{r sentiment_analysis_naive_bayes, include = T, echo = F}
  
  # Create Document Term Matrix.
  # I do this from 'wordcloud_tweet_corpus' as this corpus already has the pre-processing/cleaning I want.
  # I impose word length and term frequency bounds on the terms in the document term matrix.
  tweet_tdm = as.matrix(DocumentTermMatrix(wordcloud_tweet_corpus, 
                                           control = list(wordLengths = c(3, nrow(tweet_data)), 
                                           bounds = list(global = c(5, nrow(tweet_data))))))
  
  # Turn 'tweet_tdm' into a 'data.table'.
  tweet_tdm = as.data.frame(tweet_tdm)
  
  # Add true sentiment analysis classifications to 'tweet_tdm'.
  tweet_tdm$airline_sentiment = tweet_data$airline_sentiment
  
  # Create training and test sets from 'tweet_tdm'.
  # Both training and test sets contain approximately half of the data.
  
    # Set seed and randomly sample the row numbers for the training set.
    set.seed(1)
    training_rows = sample(1:nrow(tweet_data), 
                           nrow(tweet_data) / 2, 
                           replace = F)
  
    # Create training and test sets.
    tweet_tdm_train = tweet_tdm[training_rows, ]
    tweet_tdm_test = tweet_tdm[(-training_rows), ]
    
  # Do feature selection.
    
    # Compute feature importance using information gain on training data.
    train_feature_importance = information.gain(airline_sentiment ~., 
                         data = tweet_tdm_train)
    
    # Limit 'train_feature_importance' to only include features with an importance greater than 0.
    train_feature_importance = subset(train_feature_importance, 
                                      attr_importance > 0)
    
    # The relevant features are contained in the row names of 'train_feature_importance', which I name 'relevant_features'.
    # The first entry of 'train_feature_importance' has some special characters which somehow made it through to this point.
    # This entry wreaks havoc and so I remove it.
    # I also add 'airline_sentiment' to 'relevant_features', because these answers are necessary for model building.
    relevant_features = c(row.names(train_feature_importance)[2:length(row.names(train_feature_importance))], 'airline_sentiment')
    
    # Limit training and test data sets to the relevant columns/features.
    # Note that I calculated feature importance on the training set only and used the most important features in both the test and training set.
    tweet_tdm_train = tweet_tdm_train[, relevant_features]
    tweet_tdm_test = tweet_tdm_test[, relevant_features]
    
  # Change 'airline_sentiment' to factor for training and test data to allow model to run.  
  tweet_tdm_train$airline_sentiment = as.factor(tweet_tdm_train$airline_sentiment)
  tweet_tdm_test$airline_sentiment = as.factor(tweet_tdm_test$airline_sentiment)
    
  # Create model on training data.
  naive_bayes_model = naiveBayes(formula = airline_sentiment ~., 
                                 data = tweet_tdm_train, 
                                 laplace = 1)
    
  # Predict values for training and test sets.
  training_pred = predict(naive_bayes_model, 
                             newdata = tweet_tdm_train, 
                             type = 'class')
  test_pred = predict(naive_bayes_model, 
                      newdata = tweet_tdm_test, 
                      type = 'class')
  
  # Add predictions to training and test sets.
  tweet_tdm_train$predicted_sentiment_nb = training_pred
  tweet_tdm_test$predicted_sentiment_nb = test_pred
  
  # Change training and test set to 'data.table' to help communication of results below.
  tweet_tdm_train = as.data.table(tweet_tdm_train)
  tweet_tdm_test = as.data.table(tweet_tdm_test)
  
  # Compute cross validation error.
  cv_error = tune(method = 'naiveBayes', 
           airline_sentiment ~., 
           data = tweet_tdm_train, 
           size = 5)
  
  # Note: in the reporting below, including the 'cv_error' from above breaks RMarkdown.
  # Adding 'print(cv_error)' seems to just be ignored.
  # So, I just manually write in the cross validation error.
  
```
After computing feature importance for each word, I include the 110 words with a non-zero (actually, greater than 0) feature importance.

Results:   
Training error: `r 100 * round(nrow(tweet_tdm_train[airline_sentiment != predicted_sentiment_nb]) / nrow(tweet_tdm_train), 4)`%   
Cross Validation error: 58.61%     
Test error: `r 100 * round(nrow(tweet_tdm_test[airline_sentiment != predicted_sentiment_nb]) / nrow(tweet_tdm_test), 4)`%   

Results by tweet classification category:   
`r nrow(tweet_tdm_test[airline_sentiment == predicted_sentiment_nb & airline_sentiment == 'positive'])` out of `r nrow(tweet_tdm_test[airline_sentiment == 'positive'])` (`r round(100 * nrow(tweet_tdm_test[airline_sentiment == predicted_sentiment_nb & airline_sentiment == 'positive']) / nrow(tweet_tdm_test[airline_sentiment == 'positive']), 2)`)% of positive tweets are classified correctly.

`r nrow(tweet_tdm_test[airline_sentiment == predicted_sentiment_nb & airline_sentiment == 'neutral'])` out of `r nrow(tweet_tdm_test[airline_sentiment == 'neutral'])` (`r round(100 * nrow(tweet_tdm_test[airline_sentiment == predicted_sentiment_nb & airline_sentiment == 'neutral']) / nrow(tweet_tdm_test[airline_sentiment == 'neutral']), 2)`)% of neutral tweets are classified correctly.

`r nrow(tweet_tdm_test[airline_sentiment == predicted_sentiment_nb & airline_sentiment == 'negative'])` out of `r nrow(tweet_tdm_test[airline_sentiment == 'negative'])` (`r round(100 * nrow(tweet_tdm_test[airline_sentiment == predicted_sentiment_nb & airline_sentiment == 'negative']) / nrow(tweet_tdm_test[airline_sentiment == 'negative']), 2)`)% of negative tweets are classified correctly.

Overall, the Naive Bayes model performs slightly better than the lexicon based model. Interestingly, the Naive Bayes model does extremely well on positive tweets and much better than the lexicon based model. However, Naive Bayes does poorly on neutral tweets and much worse than the lexicon based model. Naive Bayes does somewhat better than the lexicon based model when classifying negative tweets.

Interestingly, training, cross validation, and test set error are almost identical.

**Further Exploration:**

Better results might be achieved by using a different feature engineering process (more or less features) or by tuning some Naive Bayes model parameters (the laplace smoothing parameter is one option).