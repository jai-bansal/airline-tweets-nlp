---
title: "Airline Tweet NLP"
author: "Jai Bansal"
date: "May 18, 2016"
output: word_document
---

This document conducts basic natural language processing (NLP) analysis on Twitter tweets about major US airlines scraped from the site during part of February 2015.  
Contributors were asked to classify positive, negative, and neutral tweets, followed by categorizing negative reasons (such as "late flight" or "rude service").
Thus, for each tweet, I have the 'correct' answer for sentiment analysis purposes.

The data can be found at the URL below. To find the dataset, search for 'Airline' on the page.  
I specifically use the 16,000 row dataset uploaded on February 12, 2015 by CrowdFlower.  
I assume the upload date is incorrect as the data includes tweets from after 2/12/2015...  

https://www.crowdflower.com/data-for-everyone/

Note that the actual dataset only appears to contain 14,640 rows. I'm not sure where the discrepancy between actual rows and rows stated on the website comes from, but it doesn't affect my analysis.

```{r import_and_prep, include = F, echo = F}
  
  # Load packages.
  library(data.table)
  library(tm)
  library(SnowballC)
  library(wordcloud)
  library(tm.plugin.sentiment)
  library(e1071)

  # Import data.
  tweet_data = data.table(read.csv('Airline-Sentiment-2-w-AA.csv', 
                                  header = T, 
                                  stringsAsFactors = F))
  
  # Change tweet text to all lowercase.
  tweet_data$text = tolower(tweet_data$text)
  
```

### Word Cloud:

Below is a word cloud with the 50 most frequently used words (technically stems) in the tweet data. 
The larger and darker a word, the more frequently it was used.

```{r word_cloud, include = T, echo = F}
  
  # Create corpus of tweet data ('tweet_data$text').
  tweet_corpus = Corpus(VectorSource(tweet_data$text))
  
  # Remove punctuation from 'tweet_corpus'.
  wordcloud_tweet_corpus = tm_map(tweet_corpus, 
                                  removePunctuation)
  
  # Remove extra whitespace from 'tweet_corpus'.
  wordcloud_tweet_corpus = tm_map(tweet_corpus, 
                                  stripWhitespace)
  
  # Remove numbers from 'tweet_corpus'.
  wordcloud_tweet_corpus = tm_map(tweet_corpus, 
                                  removeNumbers)
  
  # Remove (English) stopwords and airline names from 'tweet_corpus'.
  # I want to exclude airline names in the word cloud and at least one airline name is likely to be in every tweet.
  wordcloud_tweet_corpus = tm_map(wordcloud_tweet_corpus, 
                                  removeWords, 
                                  c('virginamerica', 'united', 'southwestair', 'jetblue', 'usairways', 'americanair', 
                                    stopwords('english')))
  
  # Perform stemming on 'tweet_corpus'.
  wordcloud_tweet_corpus = tm_map(wordcloud_tweet_corpus, 
                                  stemDocument)
  
  # Plot wordcloud.
  wordcloud(wordcloud_tweet_corpus,
            max.words = 50,
            random.order = F, 
            colors = c('lightskyblue', 'cornflowerblue', 'blue', 'blue4'))
  
```

### Sentiment Analysis:

I will conduct sentiment analysis in 2 ways.  
* Lexicon based (with pre-provided lists of positive and negative terms)   
* Naive Bayes Classification Model

**Lexicon Based**:

```{r sentiment_analysis_lexicon_based, include = T, echo = F}
  
  # Conduct data cleaning and compute sentiment analysis scores for 'tweet_data'.
  # Specifically, the text cleaning that occurs is making the text lowercase, removing punctuation, 
  # removing numbers, removing (English) stopwords, removing whitespace, stemming words, and removing words with less than 3 characters.
  # I do this process on the unmodified 'tweet_corpus' instead of 'wordcloud_tweet_corpus' which has already been preprocessed to some extent.
  sa_tweet_data = score(tweet_corpus)
  
  # Get sentiment analysis scores for each tweet.
  # I list the score metric defintions below (found this info on slide 36: http://statmath.wu.ac.at/courses/SNLP/Presentations/DA-Sentiment.pdf)
  # In these definitions, N is the total number of words in a document and p[n] is the number of positive[negative] words in a document.
  # Positive and negative words are included in a pre-defined list called 'dic_gi'.
  # There are 5 score metrics computed:
  # polarity = (p - n) / (p + n)
  # subjectivity = (n + p) / N
  # pos_refs_per_ref = p / N
  # neg_refs_per_ref = n / N
  # senti_diffs_per_ref = (p - n) / N
  sa_tweet_data = meta(sa_tweet_data)
  sa_tweet_data = data.table(sa_tweet_data)
  
  # Add 'sa_tweet_data$polarity' to 'tweet_data'.
  tweet_data$polarity = sa_tweet_data$polarity
  
  # Translate polarity into a 'predicted_sentiment_lexicon_based'.
  # If 'polarity' is less[greater] than 0, the tweet will be predicted to have negative[positive] sentiment.
  # If 'polarity' is 0, the tweet will be predicted to have neutral sentiment.
  tweet_data$predicted_sentiment_lexicon_based = ifelse(tweet_data$polarity < 0, 'negative', 
                                          ifelse(tweet_data$polarity == 0, 'neutral', 'positive'))
  
```

In this section, I use a function that automatically preprocesses the data and computes various scoring metrics.  
The preprocessing that is done includes making the text lowercase, removing punctuation, removing numbers, removing (English) stopwords, removing whitespace, stemming words, and removing words with less than 3 characters.  
The scoring metric I will consider is polarity and is computed as: (p - n) / (p + n)  
p[n] is the number of positive[negative] words in a tweet. These positive and negative words come in a pre-defined list.

For each tweet, if polarity is less[greater] than 0, the tweet will be predicted to have negative[positive] sentiment.  
Tweets will a polarity of 0 will be predicted to be neutral.

Using these definitions, `r nrow(tweet_data[airline_sentiment == predicted_sentiment_lexicon_based])` out of `r nrow(tweet_data)` (`r round(100 * nrow(tweet_data[airline_sentiment == predicted_sentiment_lexicon_based]) / nrow(tweet_data), 2)`)% of tweets are predicted correctly.

Broken down by category:   
`r nrow(tweet_data[airline_sentiment == predicted_sentiment_lexicon_based & airline_sentiment == 'positive'])` out of `r nrow(tweet_data[airline_sentiment == 'positive'])` (`r round(100 * nrow(tweet_data[airline_sentiment == predicted_sentiment_lexicon_based & airline_sentiment == 'positive']) / nrow(tweet_data[airline_sentiment == 'positive']), 2)`)% of positive tweets are predicted correctly.

`r nrow(tweet_data[airline_sentiment == predicted_sentiment_lexicon_based & airline_sentiment == 'neutral'])` out of `r nrow(tweet_data[airline_sentiment == 'neutral'])` (`r round(100 * nrow(tweet_data[airline_sentiment == predicted_sentiment_lexicon_based & airline_sentiment == 'neutral']) / nrow(tweet_data[airline_sentiment == 'neutral']), 2)`)% of neutral tweets are predicted correctly.

`r nrow(tweet_data[airline_sentiment == predicted_sentiment_lexicon_based & airline_sentiment == 'negative'])` out of `r nrow(tweet_data[airline_sentiment == 'negative'])` (`r round(100 * nrow(tweet_data[airline_sentiment == predicted_sentiment_lexicon_based & airline_sentiment == 'negative']) / nrow(tweet_data[airline_sentiment == 'negative']), 2)`)% of negative tweets are predicted correctly.

Further Exploration:  

These results are not great. Results might be improved by using a different (possibly custom) list of positively and negatively associated words. Since these tweets are directed at airlines, the lists should probably include air travel specific terms. Results might also improve by using different polarity cutoffs. Perhaps tweets with scores that are slightly above/below zero should be classifed as neutral. Or maybe tweets with a slightly negative polarity should be classified as positive. There are many variations to explore here. Finally, a different scoring metric might yield better results.

**Naive Bayes Classification Model**:

```{r sentiment_analysis_naive_bayes, include = T, echo = F}
  
  # Create Term Document Matrix.
  # I do this from 'wordcloud_tweet_corpus' as this corpus already has the pre-processing/cleaning I want.
  tweet_tdm = as.matrix(DocumentTermMatrix(wordcloud_tweet_corpus))
  
  # Compute the sums of each column of 'tweet_tdm'.
  # This tells me how often each term is used.
  sums = colSums(tweet_tdm)
  
  # Get the relative order of the values in 'sums'.
  ordered = order(sums)
  
  
    
```

