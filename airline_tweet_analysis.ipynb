{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airline Tweets NLP Analysis\n",
    "\n",
    "This document shows the results of basic natural language processing (NLP) analysis on Twitter tweets about major US airlines scraped from the site during part of February 2015. Specifically, I create a word cloud and conduct sentiment analysis.\n",
    "\n",
    "Contributors to the data set were asked to classify positive, negative, and neutral tweets.\n",
    "Thus, for each tweet, I have the 'correct' answer for sentiment analysis purposes.\n",
    "\n",
    "The data can be found at the URL below. To find the dataset, search for 'Airline' on the page.  \n",
    "I specifically use the 16,000 row dataset uploaded on February 12, 2015 by CrowdFlower.  \n",
    "I assume the upload date is incorrect as the data includes tweets from after 2/12/2015...\n",
    "\n",
    "https://www.crowdflower.com/data-for-everyone/\n",
    "\n",
    "Note that the actual dataset only contains 14,640 rows. I'm not sure where the discrepancy comes from, but it doesn't affect the analysis.\n",
    "\n",
    "In the cell below, I import modules for the analysis and the data. Note that the file path is specific to my machine and may need to be modified if this code is run elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import modules.\n",
    "import pandas as pd\n",
    "import wordcloud\n",
    "from stemming.porter2 import stem\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import sklearn.naive_bayes as sklearn_nb\n",
    "from sklearn import cross_validation\n",
    "\n",
    "# Import data\n",
    "tweet_data = pd.read_csv('Documents/Github/airline-tweets-nlp-and-machine-learning/Airline-Sentiment-2-w-AA.csv', \n",
    "                         encoding = 'latin_1')\n",
    "\n",
    "# Remove unneeded columns.\n",
    "tweet_data = tweet_data[['airline_sentiment', 'text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a sample of the data. Unfortunately, in this view, we can only see the beginning of the tweet text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text\n",
       "0           neutral                @VirginAmerica What @dhepburn said.\n",
       "1          positive  @VirginAmerica plus you've added commercials t...\n",
       "2           neutral  @VirginAmerica I didn't today... Must mean I n...\n",
       "3          negative  @VirginAmerica it's really aggressive to blast...\n",
       "4          negative  @VirginAmerica and it's a really big bad thing..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View head of data.\n",
    "tweet_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, I'll do some data cleaning on 'tweet_data.text'\n",
    "- make all characters lowercase\n",
    "- remove unneeded characters\n",
    "- remove the airline Twitter handles\n",
    "- remove numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make tweets lowercase.\n",
    "tweet_data.text = tweet_data.text.str.lower()\n",
    "\n",
    "# Remove unneeded characters.\n",
    "tweet_data.text = tweet_data.text.str.replace('[^\\w\\s]', \n",
    "                                              '')\n",
    "\n",
    "# Remove airline Twitter handles.\n",
    "# Note that I have not removed stopwords.\n",
    "# This removal is done when creating the word cloud.\n",
    "# Stemming is done in the next section.\n",
    "tweet_data.text = tweet_data.text.str.replace('virginamerica', \n",
    "                                              '')\n",
    "tweet_data.text = tweet_data.text.str.replace('united', \n",
    "                                              '')\n",
    "tweet_data.text = tweet_data.text.str.replace('southwestair', \n",
    "                                              '')\n",
    "tweet_data.text = tweet_data.text.str.replace('jetblue', \n",
    "                                              '')\n",
    "tweet_data.text = tweet_data.text.str.replace('usairways', \n",
    "                                              '')\n",
    "tweet_data.text = tweet_data.text.str.replace('americanair', \n",
    "                                              '')\n",
    "\n",
    "# Remove numbers from tweet data.\n",
    "tweet_data.text = tweet_data.text.str.replace('0', \n",
    "                                              '')\n",
    "tweet_data.text = tweet_data.text.str.replace('1', \n",
    "                                              '')\n",
    "tweet_data.text = tweet_data.text.str.replace('2', \n",
    "                                              '')\n",
    "tweet_data.text = tweet_data.text.str.replace('3', \n",
    "                                              '')\n",
    "tweet_data.text = tweet_data.text.str.replace('4', \n",
    "                                              '')\n",
    "tweet_data.text = tweet_data.text.str.replace('5', \n",
    "                                              '')\n",
    "tweet_data.text = tweet_data.text.str.replace('6', \n",
    "                                              '')\n",
    "tweet_data.text = tweet_data.text.str.replace('7', \n",
    "                                              '')\n",
    "tweet_data.text = tweet_data.text.str.replace('8', \n",
    "                                              '')\n",
    "tweet_data.text = tweet_data.text.str.replace('9', \n",
    "                                              '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud:\n",
    "\n",
    "Below, I create a wordcloud of the text in the tweet data.   \n",
    "I stem the tweet data in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python35-32\\lib\\site-packages\\numpy\\ma\\core.py:3113: FutureWarning: Currently, slicing will try to return a view of the data, but will return a copy of the mask. In the future, it will try to return both as views.\n",
      "  FutureWarning\n"
     ]
    }
   ],
   "source": [
    "# For each 'tweet_data.text' create a 'split_tweet' column that is a list with 1 entry for each word.\n",
    "tweet_data['split_tweet'] = tweet_data.text.str.split(' ')\n",
    "\n",
    "# For each row in 'tweet_data', create a 'stemmed_text' column.\n",
    "# This column is a string of the stemmed words in 'tweet_data.split_tweet'.\n",
    "# First, I create the column as an empty string and then I actually fill it in.\n",
    "tweet_data['stemmed_text'] = ''\n",
    "\n",
    "for i in range(0, len(tweet_data.index)):\n",
    "    \n",
    "    for j in range(0, len(tweet_data.split_tweet[i])):\n",
    "        \n",
    "        if len(tweet_data.loc[i, 'split_tweet'][j]) > 0:\n",
    "            tweet_data.stemmed_text[i] = tweet_data.stemmed_text[i] + ' ' + stem(tweet_data.loc[i, 'split_tweet'][j])\n",
    "            \n",
    "# Turn 'tweet_data.stemmed_text' into a string for purposes of creating the word cloud.\n",
    "stemmed_text_string = str(tweet_data.stemmed_text)\n",
    "\n",
    "# Create word cloud of the top 50 words (technically stems) in the tweet data (and remove stopwords).\n",
    "# The wordcloud might look a bit odd because I'm using stemmed words.\n",
    "tweet_data_wordcloud = wordcloud.WordCloud(background_color = 'black', \n",
    "                                max_words = 50, \n",
    "                                stopwords = wordcloud.STOPWORDS)\n",
    "tweet_data_wordcloud.generate(stemmed_text_string)\n",
    "plt.imshow(tweet_data_wordcloud)\n",
    "plt.axis(\"off\") # Remove graph axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output doesn't show up here, but the PNG file in the repository named 'airline_tweet_analysis_wordcloud.png' contains the wordcloud resulting from the code above. The larger a word, the more frequently it appears in the tweet data. Some of the terms in the word cloud may look odd because I'm using stemmed words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis:\n",
    "\n",
    "I conduct sentiment analysis in 2 ways:  \n",
    "* Lexicon based (with custom lists of positive and negative terms)   \n",
    "* Naive Bayes Classification Model\n",
    "\n",
    "#### Lexicon Based:\n",
    "\n",
    "First, I create my own custom sets of positively and negatively associated words and use these for lexicon-based sentiment analysis. These lists are created using intuition and looking at some example tweets (so slight cheating/overfitting).\n",
    "\n",
    "I will then count the number of positive and negative words in each tweet and use these counts to create a score to classify the tweets as 'positive', 'negative', or 'neutral'. Then, I'll compare my classification to the provided \"answers\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create sets of positively and negatively associated terms.\n",
    "positive_terms = {'amazing', 'good', 'great', 'awesome', 'thank', 'thanks', 'love', 'excited', 'amazing', 'polite', 'courteous', 'friendly', 'incredible'}\n",
    "negative_terms = {'tacky', 'aggressive', 'obnoxious', 'bad', 'delay', 'worst', 'awful', 'cancel', 'cancelled', 'shitty', 'mess', 'fantastic', 'rude', 'mean', 'unfriendly'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will work with 'tweet_data.split_tweet'. This column is the tweets split up by word. This column has already been made lowercase, had unneeded characters removed, had airline Twitter handles removed, and had numbers removed. Including stopwords does NOT affect this particular model (I'll just be counting positive and negative words in each tweet) and so I leave them in. \n",
    "\n",
    "Stemming the data obviously changes the words and so makes it difficult to create a custom list for classification. For this reason, I will not use stemmed tweet data for this model.\n",
    "\n",
    "Next, I create 'positive_words' and 'negative_words' columns that count the number of positive and negative words in each cleaned tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create 'positive_words' and 'negative_words' columns.\n",
    "# I initially set these column equal to 0 and then fill them in.\n",
    "tweet_data['positive_words'] = 0\n",
    "tweet_data['negative_words'] = 0\n",
    "\n",
    "# Count 'positive_words' and 'negative_words' in each cleaned tweet.\n",
    "# I loop through each row and use a nested loop to count positive and negative words in each 'split_tweet'.\n",
    "for i in range(0, len(tweet_data.index)):\n",
    "    \n",
    "    # Set count of positive and negative words to 0 for each row.\n",
    "    positive_count = 0\n",
    "    negative_count = 0\n",
    "    \n",
    "    for j in range(0, len(tweet_data.split_tweet[i])):\n",
    "        \n",
    "        if tweet_data.loc[i, 'split_tweet'][j] in positive_terms:\n",
    "            positive_count += 1\n",
    "        elif tweet_data.loc[i, 'split_tweet'][j] in negative_terms:\n",
    "            negative_count += 1\n",
    "    \n",
    "    tweet_data.loc[i, 'positive_words'] = positive_count\n",
    "    tweet_data.loc[i, 'negative_words'] = negative_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scoring metric I use is polarity and is computed as: (p - n) / (p + n)   \n",
    "p[n] is the number of positive[negative] words in a tweet.\n",
    "\n",
    "For each tweet, if polarity is less[greater] than 0, the tweet will be classified as negative[positive].   \n",
    "Tweets with a polarity of 0 are classified as neutral.\n",
    "Now I compute the polarity and classification for each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute 'polarity' for each tweet.\n",
    "tweet_data['polarity'] = (tweet_data.positive_words - tweet_data.negative_words) / (tweet_data.positive_words + tweet_data.negative_words)\n",
    "\n",
    "# Classify each tweet as 'postive', 'negative', or 'neutral'.\n",
    "tweet_data['lexicon_class'] = np.where(tweet_data.polarity > 0, 'positive', \n",
    "                                      np.where(tweet_data.polarity < 0, 'negative', 'neutral'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can see lexicon-based sentiment analysis model results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tweets: 14640\n",
      "Overall Accuracy: 38.41%\n",
      "Number of Positive Tweets: 2363\n",
      "Accuracy on \"positive\" tweets (according to \"answers\"): 59.42% (1404 positive tweets classified correctly)\n",
      "Number of Neutral Tweets: 3099\n",
      "Accuracy on \"neutral\" tweets (according to \"answers\"): 85.48% (2649 neutral tweets classified correctly)\n",
      "Number of Negative Tweets: 9178\n",
      "Accuracy on \"negative\" tweets (according to \"answers\"): 17.11% (1570 negative tweets classified correctly)\n"
     ]
    }
   ],
   "source": [
    "# Overall accuracy.\n",
    "\n",
    "# Print total number of tweets.\n",
    "print('Total Tweets: ' +\n",
    "     str(len(tweet_data.index)))\n",
    "\n",
    "# Compute total accuracy.\n",
    "print('Overall Accuracy: ' + \n",
    "      str(round(100 * len(tweet_data[tweet_data.airline_sentiment == tweet_data.lexicon_class]) / len(tweet_data.index), 2)) + \n",
    "     '%')\n",
    "\n",
    "# Compute accuracy by tweet classification category.\n",
    "\n",
    "# Positive tweets.\n",
    "\n",
    "# Create helper data frame.\n",
    "positive = tweet_data[tweet_data.airline_sentiment == 'positive']\n",
    "\n",
    "# Print total number of positive tweets.\n",
    "print('Number of Positive Tweets: ' +\n",
    "      str(len(positive.index)))\n",
    "\n",
    "# Print positive tweet number and accuracy.\n",
    "print('Accuracy on \"positive\" tweets (according to \"answers\"): ' +\n",
    "      str(round(100 * len(positive[positive.airline_sentiment == positive.lexicon_class]) / len(positive.index), 2)) + \n",
    "     '% (' +\n",
    "     str(len(positive[positive.airline_sentiment == positive.lexicon_class])) +\n",
    "     ' positive tweets classified correctly' +\n",
    "     ')')\n",
    "\n",
    "# Neutral tweets.\n",
    "\n",
    "# Create helper data frame.\n",
    "neutral = tweet_data[tweet_data.airline_sentiment == 'neutral']\n",
    "\n",
    "# Print total number of neutral tweets.\n",
    "print('Number of Neutral Tweets: ' + \n",
    "      str(len(neutral.index)))\n",
    "\n",
    "# Print neutral tweet number and accuracy.\n",
    "print('Accuracy on \"neutral\" tweets (according to \"answers\"): ' +\n",
    "      str(round(100 * len(neutral[neutral.airline_sentiment == neutral.lexicon_class]) / len(neutral.index), 2)) + \n",
    "     '% (' + \n",
    "     str(len(neutral[neutral.airline_sentiment == neutral.lexicon_class])) +\n",
    "      ' neutral tweets classified correctly' +\n",
    "     ')')\n",
    "\n",
    "# Negative tweets.\n",
    "\n",
    "# Create helper data frame.\n",
    "negative = tweet_data[tweet_data.airline_sentiment == 'negative']\n",
    "\n",
    "# Print total number of negative tweets.\n",
    "print('Number of Negative Tweets: ' + \n",
    "      str(len(negative.index)))\n",
    "\n",
    "# Print negative tweet number and accuracy.\n",
    "print('Accuracy on \"negative\" tweets (according to \"answers\"): ' +\n",
    "      str(round(100 * len(negative[negative.airline_sentiment == negative.lexicon_class]) / len(negative.index), 2)) +\n",
    "     '% (' + \n",
    "     str(len(negative[negative.airline_sentiment == negative.lexicon_class])) + \n",
    "      ' negative tweets classified correctly' +\n",
    "     ')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This version of lexicon based sentiment analysis is actually more accurate over the entire data set than the version used in the R branch. In that version, I used a pre-provided list of positively and negatively associated terms.\n",
    "\n",
    "**Further Exploration**:\n",
    "\n",
    "These results are not great. They might improve if I expanded the list of positive/negative terms I used. Also, results might improve by using different classification thresholds. Actually, the way I did classification above does not take into account the denominator of the polarity at all. Finally, using a different scoring metric might yield better results.\n",
    "\n",
    "#### Naive Bayes Classification Model:\n",
    "\n",
    "I use 'tweet_data.stemmed_text' in this section. In a previous section, this column was made lowercase, had unneeded characters removed, had airline tweet handles removed, had numbers removed, and was stemmed. Below, I remove English stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean up unneeded columns ('positive_words', 'negative_words', 'polarity').\n",
    "# This command can be optionally removed with no adverse effects.\n",
    "tweet_data = tweet_data[['airline_sentiment', 'text', 'split_tweet', 'stemmed_text', 'lexicon_class']]\n",
    "\n",
    "# Create document term matrix from 'tweet_data'.\n",
    "# In the R branch, I use information gain for each feature to do feature selection.\n",
    "# In that branch, features giving no information are removed.\n",
    "# In this branch, I do not compute information gain.\n",
    "\n",
    "# Create 'count_vectorizer'.\n",
    "# This is necessary to create the document term matrix.\n",
    "# I remove accents, English stopwords, and terms that appear in less than a certain number of documents.\n",
    "# I am more aggressive in removing terms because I kept hitting memory errors when converting back to a pandas data frame.\n",
    "count_vectorizer = CountVectorizer(strip_accents = 'unicode', \n",
    "                                  stop_words = 'english', \n",
    "                                  min_df = 8,\n",
    "                                  binary = True)\n",
    "\n",
    "# Create document term matrix.\n",
    "tweet_tdm = count_vectorizer.fit_transform(tweet_data.text)\n",
    "\n",
    "# Turn 'tweet_tdm' into a data frame.\n",
    "tweet_tdm = pd.DataFrame(tweet_tdm.toarray(), \n",
    "                        columns = count_vectorizer.get_feature_names())\n",
    "\n",
    "# Add 'tweet_data.airline_sentiment' to 'tweet_tdm'.\n",
    "# These are the classification task answers.\n",
    "tweet_tdm['airline_sentiment'] = tweet_data.airline_sentiment\n",
    "\n",
    "# Create training and test sets from 'tweet_data'.\n",
    "# Both training and test sets contain approximately half the data.\n",
    "\n",
    "# Set seed for reproducibility.\n",
    "np.random.seed(79)\n",
    "\n",
    "# Randomly sample index numbers for the training set.\n",
    "training_rows = np.random.choice(len(tweet_data.index), \n",
    "                                 size = int(len(tweet_data.index) / 2), \n",
    "                                 replace = False)\n",
    "\n",
    "# Create training and test sets.\n",
    "tweet_tdm_train = tweet_tdm.iloc[training_rows]\n",
    "tweet_tdm_test = tweet_tdm.iloc[-training_rows]\n",
    "\n",
    "# Create Naive Bayes classifier.\n",
    "nb = sklearn_nb.MultinomialNB()\n",
    "                \n",
    "# Train model\n",
    "nb.fit(tweet_tdm_train.drop('airline_sentiment', \n",
    "                            axis = 1), \n",
    "       tweet_tdm_train.airline_sentiment)\n",
    "\n",
    "# Conduct cross validation.\n",
    "# I can't get the built-in cross validation functions to work, so I build my own 2 fold cross validation on the training set.\n",
    "\n",
    "# Reset 'tweet_tdm_train' index.\n",
    "tweet_tdm_train.reset_index(drop = True,\n",
    "                           inplace = True)\n",
    "\n",
    "# Randomly sample index numbers for the 2 folds of cross validation (from training data).\n",
    "fold_1_rows = np.random.choice(len(tweet_tdm_train.index), \n",
    "                              size = int(len(tweet_tdm_train.index) / 2), \n",
    "                              replace = False)\n",
    "\n",
    "# Create 2 folds from training data.\n",
    "fold_1 = tweet_tdm_train.iloc[fold_1_rows]\n",
    "fold_2 = tweet_tdm_train.iloc[-fold_1_rows]\n",
    "\n",
    "# Create 2 Naive Bayes models.\n",
    "# 'nb_cv_1' trains on 'fold_1' and 'nb_cv_2' trains on 'fold_2'.\n",
    "# I will use these to compute the cross validation score later in the analysis.\n",
    "nb_cv_1 = sklearn_nb.MultinomialNB()\n",
    "nb_cv_2 = sklearn_nb.MultinomialNB()\n",
    "nb_cv_1.fit(fold_1.drop('airline_sentiment', \n",
    "                       axis = 1), \n",
    "           fold_1.airline_sentiment)\n",
    "nb_cv_2.fit(fold_2.drop('airline_sentiment', \n",
    "                       axis = 1), \n",
    "           fold_2.airline_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I remove terms that appear in less than 8 documents for the model.\n",
    "Using this filter results in approximately 1892 features being used in the Naive Bayes model.\n",
    "Below, I compute various accuracy scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy: 81.61%\n",
      "Cross Validation Accuracy: 78.32%\n",
      "Test Set Accuracy: 78.51%\n",
      "Number of Positive Tweets in test set: 1160\n",
      "Accuracy on test set \"positive\" tweets (according to \"answers\"): 72.33%\n",
      "Number of Neutral Tweets in test set: 1549\n",
      "Accuracy on test set \"neutral\" tweets (according to \"answers\"): 51.39%\n",
      "Number of Negative Tweets in test set: 4611\n",
      "Accuracy on test set \"negative\" tweets (according to \"answers\"): 89.18%\n"
     ]
    }
   ],
   "source": [
    "# Print various accuracy scores.\n",
    "\n",
    "# Print training set accuracy.\n",
    "print('Training Set Accuracy: ' + \n",
    "      str(100 * round(nb.score(tweet_tdm_train.drop('airline_sentiment', \n",
    "                                                    axis = 1), \n",
    "                               tweet_tdm_train.airline_sentiment), 4)) + \n",
    "     '%')\n",
    "\n",
    "# Print cross validation set accuracy:\n",
    "print('Cross Validation Accuracy: ' + \n",
    "     str(100 * round((nb_cv_1.score(fold_2.drop('airline_sentiment', \n",
    "                                                axis = 1), \n",
    "                                    fold_2.airline_sentiment) + \n",
    "                      nb_cv_2.score(fold_1.drop('airline_sentiment', \n",
    "                                                axis = 1), \n",
    "                                    fold_1.airline_sentiment)) / 2, 4)) + \n",
    "     '%')\n",
    "\n",
    "# Print test set accuracy.\n",
    "print('Test Set Accuracy: ' + \n",
    "      str(100 * round(nb.score(tweet_tdm_test.drop('airline_sentiment', \n",
    "                                                  axis = 1), \n",
    "                              tweet_tdm_test.airline_sentiment), 4)) + \n",
    "     '%')\n",
    "\n",
    "# For test set, compute accuracy by tweet classification category.\n",
    "\n",
    "# Positive tweets.\n",
    "\n",
    "# Create helper data frame.\n",
    "positive_test = tweet_tdm_test[tweet_tdm_test.airline_sentiment == 'positive']\n",
    "\n",
    "# Print total number of positive tweets in test set.\n",
    "print('Number of Positive Tweets in test set: ' +\n",
    "      str(len(positive_test.index)))\n",
    "\n",
    "# Print positive tweet accuracy for test set.\n",
    "print('Accuracy on test set \"positive\" tweets (according to \"answers\"): ' + \n",
    "      str(100 * round(nb.score(positive_test.drop('airline_sentiment', \n",
    "                                                 axis = 1), \n",
    "                              positive_test.airline_sentiment), 4)) + \n",
    "      '%')\n",
    "\n",
    "# Neutral tweets.\n",
    "\n",
    "# Create helper data frame.\n",
    "neutral_test = tweet_tdm_test[tweet_tdm_test.airline_sentiment == 'neutral']\n",
    "\n",
    "# Print total number of neutral tweets in test set.\n",
    "print('Number of Neutral Tweets in test set: ' + \n",
    "      str(len(neutral_test.index)))\n",
    "\n",
    "# Print neutral tweet number and accuracy for test set.\n",
    "print('Accuracy on test set \"neutral\" tweets (according to \"answers\"): ' + \n",
    "      str(100 * round(nb.score(neutral_test.drop('airline_sentiment', \n",
    "                                                 axis = 1), \n",
    "                              neutral_test.airline_sentiment), 4)) + \n",
    "      '%')\n",
    "\n",
    "# Negative tweets.\n",
    "\n",
    "# Create helper data frame.\n",
    "negative_test = tweet_tdm_test[tweet_tdm_test.airline_sentiment == 'negative']\n",
    "\n",
    "# Print total number of negative tweets in test set.\n",
    "print('Number of Negative Tweets in test set: ' + \n",
    "      str(len(negative_test.index)))\n",
    "\n",
    "# Print negative tweet number and accuracy for test set.\n",
    "print('Accuracy on test set \"negative\" tweets (according to \"answers\"): ' + \n",
    "      str(100 * round(nb.score(negative_test.drop('airline_sentiment', \n",
    "                                                 axis = 1), \n",
    "                              negative_test.airline_sentiment), 4)) + \n",
    "      '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Overall, the Naive Bayes model performs much better than the lexicon based model (and the Naive Bayes model tested in the R branch). I analyze the differences between the Python and R branch Naive Bayes model in the next section.\n",
    "\n",
    "The Naive Bayes model does better on positive tweets and substantially better on negative tweets than the lexicon based model. The lexicon based model does much better on neutral tweets.\n",
    "\n",
    "**Further Exploration:**\n",
    "\n",
    "The Naive Bayes model above has a test set accuracy of 78.51%. The Naive Bayes model tested in the R branch has a test set accuracy of 58.4%. Why the difference? One possible reason: in the R branch model, I only end up using 110 features while in the Python branch I use 1892 features. However, I chose to use 110 features in the R branch because the rest of the features had zero information gain. But it appears that in the Python branch, more features led to a better result."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
