{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airline Tweets NLP Analysis\n",
    "\n",
    "This document shows the results of basic natural language processing (NLP) analysis on Twitter tweets about major US airlines scraped from the site during part of February 2015. Specifically, I create a word cloud and conduct sentiment analysis.\n",
    "\n",
    "Contributors to the data set were asked to classify positive, negative, and neutral tweets.\n",
    "Thus, for each tweet, I have the 'correct' answer for sentiment analysis purposes.\n",
    "\n",
    "The data can be found at the URL below. To find the dataset, search for 'Airline' on the page.  \n",
    "I specifically use the 16,000 row dataset uploaded on February 12, 2015 by CrowdFlower.  \n",
    "I assume the upload date is incorrect as the data includes tweets from after 2/12/2015...\n",
    "\n",
    "https://www.crowdflower.com/data-for-everyone/\n",
    "\n",
    "Note that the actual dataset only contains 14,640 rows. I'm not sure where the discrepancy comes from, but it doesn't affect the analysis.\n",
    "\n",
    "In the cell below, I import modules for the analysis and the data. Note that the file path is specific to my machine and may need to be modified if this code is run elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import modules.\n",
    "import pandas as pd\n",
    "import wordcloud\n",
    "from stemming.porter2 import stem\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import sklearn.naive_bayes as sklearn_nb\n",
    "\n",
    "# Import data\n",
    "tweet_data = pd.read_csv('Documents/Github/airline-tweets-nlp-and-machine-learning/Airline-Sentiment-2-w-AA.csv', \n",
    "                         encoding = 'latin_1')\n",
    "\n",
    "# Remove unneeded columns.\n",
    "tweet_data = tweet_data[['airline_sentiment', 'text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a sample of the data. Unfortunately, in this view, we can only see the beginning of the tweet text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text\n",
       "0           neutral                @VirginAmerica What @dhepburn said.\n",
       "1          positive  @VirginAmerica plus you've added commercials t...\n",
       "2           neutral  @VirginAmerica I didn't today... Must mean I n...\n",
       "3          negative  @VirginAmerica it's really aggressive to blast...\n",
       "4          negative  @VirginAmerica and it's a really big bad thing..."
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View head of data.\n",
    "tweet_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, I'll do some data cleaning on 'tweet_data.text'\n",
    "- make all characters lowercase\n",
    "- remove unneeded characters\n",
    "- remove the airline Twitter handles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make tweets lowercase.\n",
    "tweet_data.text = tweet_data.text.str.lower()\n",
    "\n",
    "# Remove unneeded characters.\n",
    "tweet_data.text = tweet_data.text.str.replace('[^\\w\\s]', \n",
    "                                              '')\n",
    "\n",
    "# Remove airline Twitter handles.\n",
    "# Note that I have not removed stopwords.\n",
    "# This removal is done when creating the word cloud.\n",
    "# Stemming is done in the next section.\n",
    "tweet_data.text = tweet_data.text.str.replace('virginamerica', \n",
    "                                              '')\n",
    "tweet_data.text = tweet_data.text.str.replace('united', \n",
    "                                              '')\n",
    "tweet_data.text = tweet_data.text.str.replace('southwestair', \n",
    "                                              '')\n",
    "tweet_data.text = tweet_data.text.str.replace('jetblue', \n",
    "                                              '')\n",
    "tweet_data.text = tweet_data.text.str.replace('usairways', \n",
    "                                              '')\n",
    "tweet_data.text = tweet_data.text.str.replace('americanair', \n",
    "                                              '')\n",
    "\n",
    "# For the rest of this section, I will turn 'tweet_data.text' into a single string.\n",
    "tweet_data_string = str(tweet_data.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud:\n",
    "\n",
    "The code in this section creates a wordcloud of the text in the tweet data.   \n",
    "Now, I stem the words in 'tweet_data_string'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split tweet words by spaces.\n",
    "split_tweets = tweet_data_string.split(' ')\n",
    "\n",
    "# Create new empty list to hold stemmed 'split_tweets'.\n",
    "split_tweets_stemmed = []\n",
    "\n",
    "# Stem the words in 'split_tweets'.\n",
    "# There are empty list items, but the way I will proceed will make this irrelevant.\n",
    "for word in split_tweets:\n",
    "    split_tweets_stemmed.append(stem(word))\n",
    "    \n",
    "# Create 'stemmed_tweet_string' from 'split_tweets_stemmed'.\n",
    "stemmed_tweet_string = ''\n",
    "for word in split_tweets_stemmed:\n",
    "    stemmed_tweet_string = stemmed_tweet_string + str(word) + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python35-32\\lib\\site-packages\\numpy\\ma\\core.py:3113: FutureWarning: Currently, slicing will try to return a view of the data, but will return a copy of the mask. In the future, it will try to return both as views.\n",
      "  FutureWarning\n"
     ]
    }
   ],
   "source": [
    "# Create word cloud of the top 50 words (technically stems) in the tweet data (and remove stopwords).\n",
    "tweet_data_wordcloud = wordcloud.WordCloud(background_color = 'black', \n",
    "                                max_words = 50, \n",
    "                                stopwords = wordcloud.STOPWORDS)\n",
    "tweet_data_wordcloud.generate(stemmed_tweet_string)\n",
    "plt.imshow(tweet_data_wordcloud)\n",
    "plt.axis(\"off\") # Remove graph axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output doesn't show up here, but the PNG file in the repository named 'airline_tweet_analysis_wordcloud.png' contains the wordcloud resulting from the code above. The larger a word, the more frequently it appears in the tweet data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis:\n",
    "\n",
    "I conduct sentiment analysis in 2 ways:  \n",
    "* Lexicon based (with pre-provided lists of positive and negative terms)   \n",
    "* Naive Bayes Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lexicon Based:\n",
    "\n",
    "First, I create my own custom sets of positively and negatively associated words and use these for lexicon-based sentiment analysis. These lists are created using intuition and looking at some example tweets (so slight cheating/overfitting).\n",
    "\n",
    "I will then count the number of positive and negative words in each tweet and use these counts to create a score to classify the tweets as 'positive', 'negative', or 'neutral'. Then, I'll compare my classification to the provided \"answers\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create sets of positively and negatively associated terms.\n",
    "positive_terms = {'amazing', 'good', 'great', 'awesome', 'thank', 'thanks', 'love', 'excited', 'amazing', 'polite', 'courteous', 'friendly', 'incredible'}\n",
    "negative_terms = {'tacky', 'aggressive', 'obnoxious', 'bad', 'delay', 'worst', 'awful', 'cancel', 'cancelled', 'shitty', 'mess', 'fantastic', 'rude', 'mean', 'unfriendly'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will work with 'tweet_data.text'. This data has already been made lowercase, had unneeded characters removed, and had airline Twitter handles removed. Including stopwords does NOT affect this particular model (I'll just be counting positive and negative words in each tweet) and so I leave them in. \n",
    "\n",
    "Stemming the data obviously changes the words and so makes it difficult to create a custom list for classification. For this reason, I will not stem the tweet data for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For each 'tweet_data.text' create a 'split_tweet' column that is a list with 1 entry for each word.\n",
    "# The resulting lists contain empty values, but this does not matter for this analysis.\n",
    "tweet_data['split_tweet'] = tweet_data.text.str.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 'positive_words' and 'negative_words' columns that count the number of positive and negative words in each cleaned tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create 'positive_words' and 'negative_words' columns.\n",
    "# These columns are initially populated with 'NA' and are correctly filled in below.\n",
    "tweet_data['positive_words'] = 0\n",
    "tweet_data['negative_words'] = 0\n",
    "\n",
    "# Count 'positive_words' and 'negative_words' in each cleaned tweet.\n",
    "# I loop through each row and use a nested loop to count positive and negative words in each 'split_tweet'.\n",
    "# This step also takes a little while.\n",
    "for i in range(0, len(tweet_data.text)):\n",
    "    \n",
    "    # Set count of positive and negative words to 0 for each row.\n",
    "    positive_count = 0\n",
    "    negative_count = 0\n",
    "    \n",
    "    for j in range(0, len(tweet_data.split_tweet[i])):\n",
    "        \n",
    "        if tweet_data.split_tweet[i][j] in positive_terms:\n",
    "            positive_count += 1\n",
    "        elif tweet_data.split_tweet[i][j] in negative_terms:\n",
    "            negative_count += 1\n",
    "    \n",
    "    tweet_data.loc[i, 'positive_words'] = positive_count\n",
    "    tweet_data.loc[i, 'negative_words'] = negative_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scoring metric I use is polarity and is computed as: (p - n) / (p + n)   \n",
    "p[n] is the number of positive[negative] words in a tweet.\n",
    "\n",
    "For each tweet, if polarity is less[greater] than 0, the tweet will be classified as negative[positive].   \n",
    "Tweets with a polarity of 0 are classified as neutral.\n",
    "Now I compute the polarity and classification for each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute 'polarity' for each tweet.\n",
    "tweet_data['polarity'] = (tweet_data.positive_words - tweet_data.negative_words) / (tweet_data.positive_words + tweet_data.negative_words)\n",
    "\n",
    "# Classify each tweet as 'postive', 'negative', or 'neutral'.\n",
    "tweet_data['lexicon_class'] = np.where(tweet_data.polarity > 0, 'positive', \n",
    "                                      np.where(tweet_data.polarity < 0, 'negative', 'neutral'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can see lexicon-based sentiment analysis model results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tweets: 14640\n",
      "Overall Accuracy: 38.41%\n",
      "Number of Positive Tweets: 2363\n",
      "Accuracy on \"positive\" tweets (according to \"answers\"): 59.42% (1404 positive tweets classified correctly)\n",
      "Number of Neutral Tweets: 3099\n",
      "Accuracy on \"neutral\" tweets (according to \"answers\"): 85.48% (2649 neutral tweets classified correctly)\n",
      "Number of Negative Tweets: 9178\n",
      "Accuracy on \"negative\" tweets (according to \"answers\"): 17.11% (1570 negative tweets classified correctly)\n"
     ]
    }
   ],
   "source": [
    "# Overall accuracy.\n",
    "\n",
    "# Print total number of tweets.\n",
    "print('Total Tweets: ' +\n",
    "     str(len(tweet_data.index)))\n",
    "\n",
    "# Compute total accuracy.\n",
    "print('Overall Accuracy: ' + \n",
    "      str(round(100 * len(tweet_data[tweet_data.airline_sentiment == tweet_data.lexicon_class]) / len(tweet_data.index), 2)) + \n",
    "     '%')\n",
    "\n",
    "# Compute accuracy by tweet classification category.\n",
    "\n",
    "# Positive tweets.\n",
    "\n",
    "# Create helper data frame.\n",
    "positive = tweet_data[tweet_data.airline_sentiment == 'positive']\n",
    "\n",
    "# Print total number of positive tweets.\n",
    "print('Number of Positive Tweets: ' +\n",
    "      str(len(positive.index)))\n",
    "\n",
    "# Print positive tweet number and accuracy.\n",
    "print('Accuracy on \"positive\" tweets (according to \"answers\"): ' +\n",
    "      str(round(100 * len(positive[positive.airline_sentiment == positive.lexicon_class]) / len(positive.index), 2)) + \n",
    "     '% (' +\n",
    "     str(len(positive[positive.airline_sentiment == positive.lexicon_class])) +\n",
    "     ' positive tweets classified correctly' +\n",
    "     ')')\n",
    "\n",
    "# Neutral tweets.\n",
    "\n",
    "# Create helper data frame.\n",
    "neutral = tweet_data[tweet_data.airline_sentiment == 'neutral']\n",
    "\n",
    "# Print total number of neutral tweets.\n",
    "print('Number of Neutral Tweets: ' + \n",
    "      str(len(neutral.index)))\n",
    "\n",
    "# Print neutral tweet number and accuracy.\n",
    "print('Accuracy on \"neutral\" tweets (according to \"answers\"): ' +\n",
    "      str(round(100 * len(neutral[neutral.airline_sentiment == neutral.lexicon_class]) / len(neutral.index), 2)) + \n",
    "     '% (' + \n",
    "     str(len(neutral[neutral.airline_sentiment == neutral.lexicon_class])) +\n",
    "      ' neutral tweets classified correctly' +\n",
    "     ')')\n",
    "\n",
    "# Negative tweets.\n",
    "\n",
    "# Create helper data frame.\n",
    "negative = tweet_data[tweet_data.airline_sentiment == 'negative']\n",
    "\n",
    "# Print total number of negative tweets.\n",
    "print('Number of Negative Tweets: ' + \n",
    "      str(len(negative.index)))\n",
    "\n",
    "# Print negative tweet number and accuracy.\n",
    "print('Accuracy on \"negative\" tweets (according to \"answers\"): ' +\n",
    "      str(round(100 * len(negative[negative.airline_sentiment == negative.lexicon_class]) / len(negative.index), 2)) +\n",
    "     '% (' + \n",
    "     str(len(negative[negative.airline_sentiment == negative.lexicon_class])) + \n",
    "      ' negative tweets classified correctly' +\n",
    "     ')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This version of lexicon based sentiment analysis is actually more accurate over the entire data set than the version used in the R branch. In that version, I used a pre-provided list of positively and negatively associated terms.\n",
    "\n",
    "**Further Exploration**:\n",
    "\n",
    "These results are not great. They might improve if I expanded the list of positive/negative terms I used. Also, results might improve by using different classification thresholds. Actually, the way I did classification above does not take into account the denominator of the polarity at all. Finally, using a different scoring metric might yield better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes Classification Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use 'tweet_data.text' in this section. In a previous section, 'tweet_data.text' was made lowercase, had unneeded characters removed, and had the airline tweet handles removed. Below, I remove English stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>split_tweet</th>\n",
       "      <th>lexicon_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>what dhepburn said</td>\n",
       "      <td>[, what, dhepburn, said]</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>plus youve added commercials to the experienc...</td>\n",
       "      <td>[, plus, youve, added, commercials, to, the, e...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>i didnt today must mean i need to take anothe...</td>\n",
       "      <td>[, i, didnt, today, must, mean, i, need, to, t...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>its really aggressive to blast obnoxious ente...</td>\n",
       "      <td>[, its, really, aggressive, to, blast, obnoxio...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>and its a really big bad thing about it</td>\n",
       "      <td>[, and, its, a, really, big, bad, thing, about...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text  \\\n",
       "0           neutral                                 what dhepburn said   \n",
       "1          positive   plus youve added commercials to the experienc...   \n",
       "2           neutral   i didnt today must mean i need to take anothe...   \n",
       "3          negative   its really aggressive to blast obnoxious ente...   \n",
       "4          negative            and its a really big bad thing about it   \n",
       "\n",
       "                                         split_tweet lexicon_class  \n",
       "0                           [, what, dhepburn, said]       neutral  \n",
       "1  [, plus, youve, added, commercials, to, the, e...      negative  \n",
       "2  [, i, didnt, today, must, mean, i, need, to, t...      negative  \n",
       "3  [, its, really, aggressive, to, blast, obnoxio...      negative  \n",
       "4  [, and, its, a, really, big, bad, thing, about...      negative  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean up unneeded columns ('positive_words', 'negative_words', 'polarity').\n",
    "# These commands can be optionally removed with no adverse effects.\n",
    "tweet_data = tweet_data[['airline_sentiment', 'text', 'split_tweet', 'lexicon_class']]\n",
    "\n",
    "# Create document term matrix from 'tweet_data'.\n",
    "\n",
    "# Create 'count_vectorizer'.\n",
    "# This is necessary to create the document term matrix.\n",
    "# I remove accents, English stopwords, and terms that appear in less than 5 documents.\n",
    "count_vectorizer = CountVectorizer(strip_accents = 'unicode', \n",
    "                                  stop_words = 'english', \n",
    "                                  min_df = 5, \n",
    "                                  binary = True)\n",
    "\n",
    "# Create document term matrix.\n",
    "tweet_tdm = count_vectorizer.fit_transform(tweet_data.text)\n",
    "tweet_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14640"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Naive Bayes classifier.\n",
    "nb = sklearn_nb.MultinomialNB()\n",
    "\n",
    "# Create training and test sets from 'tweet_data'.\n",
    "# Both training and test sets contain approximately half the data.\n",
    "\n",
    "# Set seed for reproducibility.\n",
    "np.random.seed(79)\n",
    "\n",
    "# Randomly sample index numbers for the training set.\n",
    "training_rows = np.random.choice(len(tweet_data.index), \n",
    "                                 size = int(len(tweet_data.index) / 2), \n",
    "                                 replace = False)\n",
    "\n",
    "# Create training and test sets.\n",
    "tweet_tdm_train = tweet_data.iloc[training_rows]\n",
    "tweet_tdm_test = tweet_data.iloc[-training_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'split_tweet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-bf72210a13ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# I loop through each row, and then stem each element of 'split_tweet' in that row (via a nested loop).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit_tweet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mtweet_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit_tweet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit_tweet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python35-32\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2358\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2359\u001b[0m             raise AttributeError(\"'%s' object has no attribute '%s'\" %\n\u001b[1;32m-> 2360\u001b[1;33m                                  (type(self).__name__, name))\n\u001b[0m\u001b[0;32m   2361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'split_tweet'"
     ]
    }
   ],
   "source": [
    "# Stem each word in each list of 'tweet_data.split_tweet'. This takes a little while.\n",
    "# I loop through each row, and then stem each element of 'split_tweet' in that row (via a nested loop).\n",
    "for i in range(0, len(tweet_data.text)):\n",
    "    for j in range(0, len(tweet_data.split_tweet[i])):\n",
    "        tweet_data.split_tweet[i][j] = stem(tweet_data.split_tweet[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
