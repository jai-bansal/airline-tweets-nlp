{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airline Tweets NLP Analysis\n",
    "\n",
    "This document shows the results of basic natural language processing (NLP) analysis on Twitter tweets about major US airlines scraped from the site during part of February 2015. Specifically, I create a word cloud and conduct sentiment analysis.\n",
    "\n",
    "Contributors to the data set were asked to classify positive, negative, and neutral tweets.\n",
    "Thus, for each tweet, I have the 'correct' answer for sentiment analysis purposes.\n",
    "\n",
    "The data can be found at the URL below. To find the dataset, search for 'Airline' on the page.  \n",
    "I specifically use the 16,000 row dataset uploaded on February 12, 2015 by CrowdFlower.  \n",
    "I assume the upload date is incorrect as the data includes tweets from after 2/12/2015...\n",
    "\n",
    "https://www.crowdflower.com/data-for-everyone/\n",
    "\n",
    "Note that the actual dataset only contains 14,640 rows. I'm not sure where the discrepancy comes from, but it doesn't affect the analysis.\n",
    "\n",
    "In the cell below, I import modules for the analysis and the data. Note that the file path is specific to my machine and may need to be modified if this code is run elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import modules.\n",
    "import pandas as pd\n",
    "import wordcloud\n",
    "from stemming.porter2 import stem\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import sklearn.naive_bayes as sklearn_nb\n",
    "\n",
    "# Import data\n",
    "tweet_data = pd.read_csv('Documents/Github/airline-tweets-nlp-and-machine-learning/Airline-Sentiment-2-w-AA.csv', \n",
    "                         encoding = 'latin_1')\n",
    "\n",
    "# Remove unneeded columns.\n",
    "tweet_data = tweet_data[['airline_sentiment', 'text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a sample of the data. Unfortunately, in this view, we can only see the beginning of the tweet text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text\n",
       "0           neutral                @VirginAmerica What @dhepburn said.\n",
       "1          positive  @VirginAmerica plus you've added commercials t...\n",
       "2           neutral  @VirginAmerica I didn't today... Must mean I n...\n",
       "3          negative  @VirginAmerica it's really aggressive to blast...\n",
       "4          negative  @VirginAmerica and it's a really big bad thing..."
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View head of data.\n",
    "tweet_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, I'll do some data cleaning on 'tweet_data.text'\n",
    "- make all characters lowercase\n",
    "- remove unneeded characters\n",
    "- remove the airline Twitter handles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make tweets lowercase.\n",
    "tweet_data.text = tweet_data.text.str.lower()\n",
    "\n",
    "# Remove unneeded characters.\n",
    "tweet_data.text = tweet_data.text.str.replace('[^\\w\\s]', \n",
    "                                              '')\n",
    "\n",
    "# Remove airline Twitter handles.\n",
    "# Note that I have not removed stopwords.\n",
    "# This removal is done when creating the word cloud.\n",
    "# Stemming is done in the next section.\n",
    "tweet_data.text = tweet_data.text.str.replace('virginamerica', \n",
    "                                              '')\n",
    "tweet_data.text = tweet_data.text.str.replace('united', \n",
    "                                              '')\n",
    "tweet_data.text = tweet_data.text.str.replace('southwestair', \n",
    "                                              '')\n",
    "tweet_data.text = tweet_data.text.str.replace('jetblue', \n",
    "                                              '')\n",
    "tweet_data.text = tweet_data.text.str.replace('usairways', \n",
    "                                              '')\n",
    "tweet_data.text = tweet_data.text.str.replace('americanair', \n",
    "                                              '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud:\n",
    "\n",
    "The code in this section creates a wordcloud of the text in the tweet data.   \n",
    "Now, I stem the words in 'tweet_data_string'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For each 'tweet_data.text' create a 'split_tweet' column that is a list with 1 entry for each word.\n",
    "# The resulting lists contain empty values.\n",
    "tweet_data['split_tweet'] = tweet_data.text.str.split(' ')\n",
    "\n",
    "# For each row in 'tweet_data', create a 'stemmed_text' column.\n",
    "# This column is a string of the stemmed words in 'tweet_data.split_tweet'.\n",
    "# First, I create the column as an empty string and then I actually fill it in.\n",
    "tweet_data['stemmed_text'] = ''\n",
    "\n",
    "for i in range(0, len(tweet_data.index)):\n",
    "    \n",
    "    for j in range(0, len(tweet_data.split_tweet[i])):\n",
    "        \n",
    "        if len(tweet_data.loc[i, 'split_tweet'][j]) > 0:\n",
    "            tweet_data.stemmed_text[i] = tweet_data.stemmed_text[i] + ' ' + stem(tweet_data.loc[i, 'split_tweet'][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python35-32\\lib\\site-packages\\numpy\\ma\\core.py:3113: FutureWarning: Currently, slicing will try to return a view of the data, but will return a copy of the mask. In the future, it will try to return both as views.\n",
      "  FutureWarning\n"
     ]
    }
   ],
   "source": [
    "# Turn 'tweet_data.stemmed_text' into a string for purposes of creating the word cloud.\n",
    "stemmed_text_string = str(tweet_data.stemmed_text)\n",
    "\n",
    "# Create word cloud of the top 50 words (technically stems) in the tweet data (and remove stopwords).\n",
    "# The wordcloud might look a bit odd because I'm using stemmed words.\n",
    "tweet_data_wordcloud = wordcloud.WordCloud(background_color = 'black', \n",
    "                                max_words = 50, \n",
    "                                stopwords = wordcloud.STOPWORDS)\n",
    "tweet_data_wordcloud.generate(stemmed_text_string)\n",
    "plt.imshow(tweet_data_wordcloud)\n",
    "plt.axis(\"off\") # Remove graph axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output doesn't show up here, but the PNG file in the repository named 'airline_tweet_analysis_wordcloud.png' contains the wordcloud resulting from the code above. The larger a word, the more frequently it appears in the tweet data. Some of the terms in the word cloud may look odd because I'm using stemmed words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis:\n",
    "\n",
    "I conduct sentiment analysis in 2 ways:  \n",
    "* Lexicon based (with pre-provided lists of positive and negative terms)   \n",
    "* Naive Bayes Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lexicon Based:\n",
    "\n",
    "First, I create my own custom sets of positively and negatively associated words and use these for lexicon-based sentiment analysis. These lists are created using intuition and looking at some example tweets (so slight cheating/overfitting).\n",
    "\n",
    "I will then count the number of positive and negative words in each tweet and use these counts to create a score to classify the tweets as 'positive', 'negative', or 'neutral'. Then, I'll compare my classification to the provided \"answers\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create sets of positively and negatively associated terms.\n",
    "positive_terms = {'amazing', 'good', 'great', 'awesome', 'thank', 'thanks', 'love', 'excited', 'amazing', 'polite', 'courteous', 'friendly', 'incredible'}\n",
    "negative_terms = {'tacky', 'aggressive', 'obnoxious', 'bad', 'delay', 'worst', 'awful', 'cancel', 'cancelled', 'shitty', 'mess', 'fantastic', 'rude', 'mean', 'unfriendly'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will work with 'tweet_data.split_tweet'. This column is the tweets split up by word. This column has already been made lowercase, had unneeded characters removed, and had airline Twitter handles removed. Including stopwords does NOT affect this particular model (I'll just be counting positive and negative words in each tweet) and so I leave them in. \n",
    "\n",
    "Stemming the data obviously changes the words and so makes it difficult to create a custom list for classification. For this reason, I will not use stemmed tweet data for this model.\n",
    "\n",
    "Next, I create 'positive_words' and 'negative_words' columns that count the number of positive and negative words in each cleaned tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create 'positive_words' and 'negative_words' columns.\n",
    "# These columns are initially populated with 'NA' and are correctly filled in below.\n",
    "tweet_data['positive_words'] = 0\n",
    "tweet_data['negative_words'] = 0\n",
    "\n",
    "# Count 'positive_words' and 'negative_words' in each cleaned tweet.\n",
    "# I loop through each row and use a nested loop to count positive and negative words in each 'split_tweet'.\n",
    "# This step also takes a little while.\n",
    "for i in range(0, len(tweet_data.index)):\n",
    "    \n",
    "    # Set count of positive and negative words to 0 for each row.\n",
    "    positive_count = 0\n",
    "    negative_count = 0\n",
    "    \n",
    "    for j in range(0, len(tweet_data.split_tweet[i])):\n",
    "        \n",
    "        if tweet_data.loc[i, 'split_tweet'][j] in positive_terms:\n",
    "            positive_count += 1\n",
    "        elif tweet_data.loc[i, 'split_tweet'][j] in negative_terms:\n",
    "            negative_count += 1\n",
    "    \n",
    "    tweet_data.loc[i, 'positive_words'] = positive_count\n",
    "    tweet_data.loc[i, 'negative_words'] = negative_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scoring metric I use is polarity and is computed as: (p - n) / (p + n)   \n",
    "p[n] is the number of positive[negative] words in a tweet.\n",
    "\n",
    "For each tweet, if polarity is less[greater] than 0, the tweet will be classified as negative[positive].   \n",
    "Tweets with a polarity of 0 are classified as neutral.\n",
    "Now I compute the polarity and classification for each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute 'polarity' for each tweet.\n",
    "tweet_data['polarity'] = (tweet_data.positive_words - tweet_data.negative_words) / (tweet_data.positive_words + tweet_data.negative_words)\n",
    "\n",
    "# Classify each tweet as 'postive', 'negative', or 'neutral'.\n",
    "tweet_data['lexicon_class'] = np.where(tweet_data.polarity > 0, 'positive', \n",
    "                                      np.where(tweet_data.polarity < 0, 'negative', 'neutral'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can see lexicon-based sentiment analysis model results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tweets: 14640\n",
      "Overall Accuracy: 38.41%\n",
      "Number of Positive Tweets: 2363\n",
      "Accuracy on \"positive\" tweets (according to \"answers\"): 59.42% (1404 positive tweets classified correctly)\n",
      "Number of Neutral Tweets: 3099\n",
      "Accuracy on \"neutral\" tweets (according to \"answers\"): 85.48% (2649 neutral tweets classified correctly)\n",
      "Number of Negative Tweets: 9178\n",
      "Accuracy on \"negative\" tweets (according to \"answers\"): 17.11% (1570 negative tweets classified correctly)\n"
     ]
    }
   ],
   "source": [
    "# Overall accuracy.\n",
    "\n",
    "# Print total number of tweets.\n",
    "print('Total Tweets: ' +\n",
    "     str(len(tweet_data.index)))\n",
    "\n",
    "# Compute total accuracy.\n",
    "print('Overall Accuracy: ' + \n",
    "      str(round(100 * len(tweet_data[tweet_data.airline_sentiment == tweet_data.lexicon_class]) / len(tweet_data.index), 2)) + \n",
    "     '%')\n",
    "\n",
    "# Compute accuracy by tweet classification category.\n",
    "\n",
    "# Positive tweets.\n",
    "\n",
    "# Create helper data frame.\n",
    "positive = tweet_data[tweet_data.airline_sentiment == 'positive']\n",
    "\n",
    "# Print total number of positive tweets.\n",
    "print('Number of Positive Tweets: ' +\n",
    "      str(len(positive.index)))\n",
    "\n",
    "# Print positive tweet number and accuracy.\n",
    "print('Accuracy on \"positive\" tweets (according to \"answers\"): ' +\n",
    "      str(round(100 * len(positive[positive.airline_sentiment == positive.lexicon_class]) / len(positive.index), 2)) + \n",
    "     '% (' +\n",
    "     str(len(positive[positive.airline_sentiment == positive.lexicon_class])) +\n",
    "     ' positive tweets classified correctly' +\n",
    "     ')')\n",
    "\n",
    "# Neutral tweets.\n",
    "\n",
    "# Create helper data frame.\n",
    "neutral = tweet_data[tweet_data.airline_sentiment == 'neutral']\n",
    "\n",
    "# Print total number of neutral tweets.\n",
    "print('Number of Neutral Tweets: ' + \n",
    "      str(len(neutral.index)))\n",
    "\n",
    "# Print neutral tweet number and accuracy.\n",
    "print('Accuracy on \"neutral\" tweets (according to \"answers\"): ' +\n",
    "      str(round(100 * len(neutral[neutral.airline_sentiment == neutral.lexicon_class]) / len(neutral.index), 2)) + \n",
    "     '% (' + \n",
    "     str(len(neutral[neutral.airline_sentiment == neutral.lexicon_class])) +\n",
    "      ' neutral tweets classified correctly' +\n",
    "     ')')\n",
    "\n",
    "# Negative tweets.\n",
    "\n",
    "# Create helper data frame.\n",
    "negative = tweet_data[tweet_data.airline_sentiment == 'negative']\n",
    "\n",
    "# Print total number of negative tweets.\n",
    "print('Number of Negative Tweets: ' + \n",
    "      str(len(negative.index)))\n",
    "\n",
    "# Print negative tweet number and accuracy.\n",
    "print('Accuracy on \"negative\" tweets (according to \"answers\"): ' +\n",
    "      str(round(100 * len(negative[negative.airline_sentiment == negative.lexicon_class]) / len(negative.index), 2)) +\n",
    "     '% (' + \n",
    "     str(len(negative[negative.airline_sentiment == negative.lexicon_class])) + \n",
    "      ' negative tweets classified correctly' +\n",
    "     ')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This version of lexicon based sentiment analysis is actually more accurate over the entire data set than the version used in the R branch. In that version, I used a pre-provided list of positively and negatively associated terms.\n",
    "\n",
    "**Further Exploration**:\n",
    "\n",
    "These results are not great. They might improve if I expanded the list of positive/negative terms I used. Also, results might improve by using different classification thresholds. Actually, the way I did classification above does not take into account the denominator of the polarity at all. Finally, using a different scoring metric might yield better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes Classification Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use 'tweet_data.stemmed_text' in this section. In a previous section, this column was made lowercase, had unneeded characters removed, had airline tweet handles removed, and was stemmed. Below, I remove English stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-154-e84ad60e60e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# Turn 'tweet_tdm' into a data frame.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m tweet_tdm = pd.DataFrame(tweet_tdm.toarray(), \n\u001b[0m\u001b[0;32m     20\u001b[0m                         columns = count_vectorizer)\n",
      "\u001b[1;32mc:\\python35-32\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m    938\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m         \u001b[1;34m\"\"\"See the docstring for `spmatrix.toarray`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 940\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtocoo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    941\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m     \u001b[1;31m##############################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python35-32\\lib\\site-packages\\scipy\\sparse\\coo.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[1;34m\"\"\"See the docstring for `spmatrix.toarray`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m         \u001b[0mB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m         \u001b[0mfortran\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfortran\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python35-32\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m    815\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 817\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    819\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__numpy_ufunc__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Clean up unneeded columns ('positive_words', 'negative_words', 'polarity').\n",
    "# This command can be optionally removed with no adverse effects.\n",
    "tweet_data = tweet_data[['airline_sentiment', 'text', 'split_tweet', 'stemmed_text', 'lexicon_class']]\n",
    "\n",
    "# Create document term matrix from 'tweet_data'.\n",
    "\n",
    "# Create 'count_vectorizer'.\n",
    "# This is necessary to create the document term matrix.\n",
    "# I remove accents, English stopwords, and terms that appear in less than 10 documents.\n",
    "# The restriction that a term must appear in at least 10 documents\n",
    "count_vectorizer = CountVectorizer(strip_accents = 'unicode', \n",
    "                                  stop_words = 'english', \n",
    "                                  min_df = 5, \n",
    "                                  binary = True)\n",
    "\n",
    "# Create document term matrix.\n",
    "tweet_tdm = count_vectorizer.fit_transform(tweet_data.text)\n",
    "\n",
    "# Turn 'tweet_tdm' into a data frame.\n",
    "tweet_tdm = pd.DataFrame(tweet_tdm.toarray(), \n",
    "                        columns = count_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14640"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Naive Bayes classifier.\n",
    "nb = sklearn_nb.MultinomialNB()\n",
    "\n",
    "# Create training and test sets from 'tweet_data'.\n",
    "# Both training and test sets contain approximately half the data.\n",
    "\n",
    "# Set seed for reproducibility.\n",
    "np.random.seed(79)\n",
    "\n",
    "# Randomly sample index numbers for the training set.\n",
    "training_rows = np.random.choice(len(tweet_data.index), \n",
    "                                 size = int(len(tweet_data.index) / 2), \n",
    "                                 replace = False)\n",
    "\n",
    "# Create training and test sets.\n",
    "tweet_tdm_train = tweet_data.iloc[training_rows]\n",
    "tweet_tdm_test = tweet_data.iloc[-training_rows]\n",
    "\n",
    "# Sample NB code.\n",
    "# Train model\n",
    "nb.fit(x, y)\n",
    "\n",
    "#predict output\n",
    "pred = nb.predict(??)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
